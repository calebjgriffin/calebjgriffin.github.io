---
title: "Exploration1"
author: "Caleb Griffin"
date: "1/31/2020"
output: pdf_document
---


```{r setup, echo=FALSE, results=FALSE, include=FALSE, cache=FALSE}
library(here)
library(tidyverse)
library(readstata13)
library(estimatr)
```


Imagine that you receive a WhatsApp from old friend. He is from one of
the new political analytics firms that started to grow during the Obama
campaign in the USA and he has a prediction problem. He has a small dataset of
8 cities and would like to predict current voting turnout in those cities based
on a complex model. He says, "My last analyst provided the following code to
fit my model but then stopped. I think that the best model of voting turnout
uses median household income, median age, racial diversity (here, percent
african american), and the number of candidates running for this city council
office. I told the analyst this model and he provided the following code. Can
you help me?"


```{r}
setwd("~/R/Quant2")
news.df <- read.csv("news.df.csv")
## Uncomment out the following line for your own use
## news.df<-read.csv("http://jakebowers.org/Data/news.df.csv")
news.df$sF<-factor(news.df$s)
```

"I really don't understand the `lsCriterion` function. Can you write out the criterion using math and explain it to me in plain language? I'm always especially interested in understanding **why** these stats types are doing this stuff, and I'm so grateful that you can explain it simply and plainly to me."

```{r}
lsCriterion<-function(b,y,X){
  yhat<-b[1]*X[,1]+b[2]*X[,2]+b[3]*X[,3]+b[4]*X[,4]+b[5]*X[,5]
  ehat<-y-yhat
  thessr<-sum(ehat^2)
  return(thessr)
}
X<-as.matrix(cbind(constant=1,news.df[,c("medhhi1000","medage","blkpct","cands")]))
y<-news.df$rpre
```

"He said to 'try some different vectors' and I think that this meant that I was to guess about the values for the coefficients in the model and that, after trying a bunch, I would choose the vector that I liked best. So, for example, I tried a model with all zeros:"

```{r}
lsCriterion(c(0,0,0,0,0,0),y=y,X=X)
```

And then tried to see a bunch of other models.

```{r}
set.seed(12345)
lsCriterion(c(1,0,4,0,0,0),y=y,X=X) ## hmm..ok... can I do better?
lsCriterion(runif(5,-100,100),y=y,X=X) ## bad
```

"After trying a bunch of models, however, I started to get tired. Can you help me do this faster? I asked the analyst (who is really a construction engineer and not a statistician) if I could use a loop but he said it would be better to 'try to optimize the objective function' and this is as far as I got."


```{r}
lsSolution<-optim(fn=lsCriterion,par=c(0,0,0,0,0),X=X,y=y,
		  method="BFGS",control=list(trace=1,REPORT=1))
```

"Is this the best solution? How well does this model predict the actual outcome (the variable ` r `) (this model uses baseline turnout or `rpre`). Can you give me some quantitative measure of how well our model predicted the outcome? I think that you can use the code from the `lsCriterion` function to develop predictions and compare our predictions to the actual turnout observed (the variable ` r`), right?"

"Now, I wanted to add another variable to see how well that new model fit. For example, maybe `blkpct` has a curvilinear relationship with the outcome. I complained to the analyst that I would have to re-write the function every time that I had a new model. So, the analyst said, 'Use matrices.' and he sent this:"

```{r}
bVector<-solve(t(X) %*% X) %*% t(X) %*% y
yhat<- X %*% bVector
ehat<-y-yhat
summary(ehat)
```

"Now, I'm very impressed at the speed and conciseness of this! I mean, he got the same vector of coefficients in like 1/1000 the time that it took me to search for a solution --- even using a fast optimizer. Also, he got the predictions very quickly too! But I'm confused about how this works. I understand the idea of proposing different values for the different coefficients and then asking how well they do --- in a sum of squared error sense. But the three lines that create `bVector` and `yhat` and even `ehat` are a mystery and I worry that they are not actually comparing my predictions of past turnout to observed future turnout (`rpre` versus ` r`). Maybe you can help? I asked the analyst to provide a little guidance to help you get practice with these ideas."

So, you need to be able to explain what is happening when we tell R to do least squares with the matrix $\bX$ and
vector $\by$ via $(\bX^{T}\bX)^{-1}\bX^{T}\by$ using the command
`solve(t(X)%*%X)%*%(t(X)%*%y)`. Where
`t(X)`$\equiv \bX^{T}$ (meaning the transpose of $\bX$) and
`solve(X)`$\equiv \bX^{-1}$ (meaning the inverse of $\bX$).

Here are some steps you might take to produce this explanation.

1.  First, let's create a $\bX$ matrix using the newspapers data to
  do a regression of the form `baseline.turnoutx~income+median age` (where
  baseline.turnout is `rpre` and income is `medhhi1000` and median age of the city is `medage`). Here is
  how one might do this in R for both $\bX$ and $\by$ (where,
  \textbf{bold} represents matrices or vectors):

```{r setupXandy, results='hide'}
X<-as.matrix(cbind(1,news.df$medhhi1000,news.df$medage))  # "bind" together columns from the data and an intercept
y<-matrix(news.df$rpre,ncol=1) # not strictly necessary, y<-news.df$rpre would also work
X
y
# Structure of the objects
str(X)
str(y)
#Look at the dimensions of the objects: number rows by number columns
dim(X)
dim(y)
```
   - Explain how we created the $\bX$ matrix \emph{Hint:} The column of 1s has to do with the intercept or constant term.
   - What do the columns of $\bX$ represent?
   - What do the rows represent?

2. First, addition and subtraction: Try each of the following lines of math in R and explain to yourself (or your colleagues) what happened, and what this means about matrix math. I did the first one as an example.

  Explain what is happening with each of the following
```{r addition,echo=TRUE,results='markup'}
X+2
```

"When you add a single number (aka a scalar) to a matrix, the scalar is added to each entry in the matrix."

Notice: If we didn't have matrix math, here is what we'd have to do to
add a scalar to a matrix
```{r add2,echo=TRUE,tidy=TRUE}
Xplus2<-matrix(NA,nrow=8,ncol=3)  # explain
for(row.entry in 1:8){ # explain
  for(col.entry in 1:3){ # explain
    Xplus2[row.entry,col.entry]<-X[row.entry,col.entry]+2 # explain
  }
}
(X+2)==Xplus2   #  explain
# An easier check on whether two objects are the same (except for
# names and such)
all.equal((X+2),Xplus2,check.attributes=FALSE)
X-2   # Explain
```

```{r vecmatadd,echo=TRUE}
twovec<-matrix(c(2,2,2),nrow=1,ncol=3)  # explain
twomat<-matrix(2,nrow=8,ncol=3)  # explain
```

You'll see some errors appear below. Your job is to explain why R
failed or made an error. I had to surround these lines in `try()`
to prevent R from stopping at the error.

```{r vecmatadd2,echo=TRUE,message=TRUE, warning=TRUE, error=TRUE}
try(X+twovec) # explain
try( X+t(twovec) )  # Here, you need to explain what t(twovec) does.
```

```{r matmat,echo=TRUE,results='hide',tidy.opts=list(keep.comment=TRUE)}
X+twomat # explain
(X+twomat)==(X+2) # explain
all.equal((X+twomat),(X+2),check.attributes=FALSE) # explain
all.equal((X+twomat),(twomat+X),check.attributes=FALSE) # explain
```

```{r self,results='hide',tidy.opts=list(keep.comment=TRUE)}
X+X # why does this work?
```

3. Second, multiplication. Notice that the symbols for scalar
  multiplication and matrix multiplication are not the same. Try each
  of the following lines of math in R and explain to yourself (or
  your colleagues) what happened, and what this means about matrix
  math.

```{r multiplication,results='hide',tidy.opts=list(keep.comment=FALSE)}
X*2  # Explain
all.equal((X*2),(2*X))   # explain
X^2  #  # explain
X^.5  # # explain
sqrt(X)  # # explain
```

Now, let's get a vector of coefficients to make matrix math link even more
tightly with what we've already done fitting models to data:

```{r}
b<-solve(t(X) %*% X) %*% t(X) %*% y  # explain
dim(b)  # explain
```

Now, let's do matrix multiplication the tedious way.  What does this
function tell us about the rule for doing matrix multiplication? Please explain the lines and the function here.

```{r tediousmult,results='hide',tidy=TRUE,tidy.opts=list(keep.comment=FALSE)}
X.times.b<-matrix(NA,nrow=8,ncol=1)  # Initialize a results matrix
for(row.entry in 1:8){  # explain
  temp<-vector(length=3) #initialize the temp vector
  for(col.entry in 1:3){  # explain
    temp[col.entry]<-X[row.entry,col.entry]*b[col.entry,]  # explain
  }
  X.times.b[row.entry,]<-sum(temp)
}
X.times.b
```

Now, doing part of it by hand: Please explain the lines of code.
```{r byhand,results='hide',tidy.opts=list(keep.comment=TRUE)}
(X[1,1] * b[1])+(X[1,2] * b[2])+(X[1,3] * b[3])
(X[2,1] * b[1])+(X[2,2] * b[2])+(X[2,3] * b[3])
(X[3,1] * b[1])+(X[3,2] * b[2])+(X[3,3] * b[3])
## etc.... for each row in X
```


And now a little faster (multiplying vectors rather than scalars and
summing): You can break matrix multiplication into separate vector
multiplication tasks since vector multiplication also goes
sum-of-row-times-column. Please explain all of this code.
```{r vectorized,results='hide',tidy.opts=list(keep.comment=FALSE)}
X[1,] %*% b # Explain
X[2,] %*% b # Explain
```

And doing it very fast: This is direct matrix multiplication. So nice
and clean compared to the previous! Don't we love matrix multiplication?
```{r fast,results='hide'}
X %*% b
```

How does `fitted(thelm)` relate to `X %*% b`? What is `%*%` in  `X %*% b` (often written $\bX \bb$ or $\bX \hat{\bbeta}$)?

```{r lmstuff}
thelm<-lm(rpre~medhhi1000+medage,data=news.df)
fitted(thelm)
```


4. How would you use matrix addition/subtraction to get the residuals once
  you had $\bX \bb$ (aka `X %*% b`)?

5. Now, let's meet another important matrix:

```{r xtx,error=TRUE,warning=TRUE,message=TRUE}
# Now another important matrix:
try(X %*% X)  # why doesn't this work.
```

```{r xtx2,results='hide'}
t(X)  # What is this?
XtX<-t(X) %*% X  # What is XtX
XtX
```

To make our lives easier, let's mean deviate or center or align all of the variables
(set it up so that all of them have mean=0). Now the `XtX`
matrix will be easier to understand:


```{r}
colmeansvec<-colMeans(X)  # What does this do?
colmeansmat<-matrix(rep(colmeansvec,8),ncol=3,byrow=TRUE)
# Fill a matrix with those means: repeat each row 8 times and stack them
```

Why would we want a matrix like the following?
```{r meancent,echo=TRUE,results='hide'}
#X-colmeansmat
# Subtract the column means from each element of X
# What is happening here?
# Or here?
#t(apply(X,1,function(x){x-colmeansvec}))
# And here is another way to do it:
#sweep(X,2,colmeansvec)  # See the help page for sweep
```

Please explain this code:

```{r xtx3,results='hide'}
X.md<-X-colmeansmat
y.md<-y-mean(y)
XtX.md<-t(X.md) %*% X.md
XtX.md
```

Explain what each entry in `XtX.md` is: if you can, relate those numbers to
quantities like variances, covariances, sums (of squares or not), sample sizes, do so.

Here another representation of `XtX` that might help you explain and reproduce
the entries above where $x_{1i}$ and $x_{2i}$ represent the two covariates in
our prediction model.

$$\bX^{T}\bX=\begin{bmatrix} n & \sum_{i = 1}^{n}{x_{1i}} & \sum_{i =
    1}^{n}{x_{2i}} \cr \sum_{i = 1}^{n}{x_{1i}} & \sum_{i = 1}^{n}
  {{x_{1i}}}^2 & \sum_{i = 1}^{n}{x_{1i}}{x_{2i}} \\ \sum_{i =
    1}^{n}{x_{2i}} & \sum_{i = 1}^{n} {x_{1i}}{x_{2i}} & \sum_{i =
    1}^{n}{{x_{2i}}}^2 \end{bmatrix}$$

Try some of the following commands to get some other help in understanding what XtX is:

```{r XtX,results='hide',tidy.opts=list(keep.comment=FALSE)}
sum(X.md[,1])
sum(X.md[,2]^2)
sum((X.md[,2]-mean(X.md[,2]))^2)  #
sum((X.md[,2]-mean(X.md[,2]))^2)/(8-1)  #
var(X.md[,2])  # Another way to get variance of z
sum(X.md[,2]*X.md[,3])  # why not use %*%? (ans: we want the cross-product for the covariance)
sum(X.md[,2]*X.md[,3])/(8-1)  #
cov(X.md)  # see the help file on cov(): The variance-covariance matrix
XtX.md/(8-1)  # What is this?
```

What about $\bX^{T} \by$? Explain the entries in `Xty.md`. Explain the code.

```{r Xty,results='hide',tidy.opts=list(keep.comment=FALSE)}
t(X.md)
y.md
Xty.md<-t(X.md) %*% y.md
Xty.md  # What is this?
cov(cbind(X.md,y.md))  #
Xty.md/7  #
```

The following is a verbal formula for a covariance:
deviations of x from its mean times deviations of y from its mean
divided by n-1 (roughly the average of the product of the deviations)
```{r xty2,results='hide'}
sum((X[,2]-mean(X[,2]))*(y-mean(y)))
sum((X[,2]-mean(X[,2]))*(y-mean(y)))/(8-1)
# Same as above because we've removed the means from X.md and y.md
sum((X.md[,2])*(y.md))/(8-1)
Xty<-t(X) %*% y
Xty
Xty/(8-1)
```

4. And finally division: Try each of the following lines of math in
  R and explain to yourself (or your colleagues) what happened
  (ideally relate what happened to ideas about variances, covariances,
  sums, etc..)

```{r div,results='hide',tidy.opts=list(keep.comment=FALSE)}
X/2  # Explain
1/X  # Explain
X^(-1)  # Same as above: 1/X==(X^(-1))
1/X==(X^(-1))
# Now matrix inversion using solve()
try(solve(X))  # Doesn't work --- why?
solve(XtX)  # This works, why?
dim(XtX)
dim(Xty)
solve(XtX)%*%Xty  # WOW! AMAZING! WONDERFUL! (what is this?)
try(solve(XtX.md))  #  What is the problem?
XtX.md<-t(X.md[,2:3])%*%X.md[,2:3]  # Hmm? What is happening?
try(solve(XtX.md))
Xty.md<-t(X.md[,2:3]) %*% y.md
solve(XtX.md) %*% Xty.md
```

```{r div2}
  # Notice that this is not the same as
(1/XtX) %*% Xty  # Matrix inversion is different from scalar division
# How does this help us?
lm(I(rpre-mean(rpre))~I(medhhi1000-mean(medhhi1000))+I(medage-mean(medage))-1,data=news.df)
# or rpre
lm(scale(rpre,scale=FALSE)~scale(medhhi1000,scale=FALSE)+scale(medage,scale=FALSE)-1,data=news.df)
# or
lm(y.md~X.md[,2]+X.md[,3]-1)
coef(lm(rpre~medhhi1000+medage,data=news.df))
```
6. So, the vector of least squares coefficients is the result of
  dividing what kind of matrix by what kind of matrix? (what kind of
  information by what kind of information)? \emph{Hint:} This
  perspective of "accounting for covariation" is another valid way
  to think about what least squares is doing [in addition to smoothing
  conditional means]. They are mathematically equivalent.

Why should covariances divided by variances amount to differences of
means, let alone adjusted differences of means?

Here are some definitions of covariance and variance:

$$\cov(X,Y)=\frac{\sum_{i}^{n}(X_i - \bar{X})(Y_i - \bar{Y})}{n-1} $$
$$\var(X)=\cov(X,X)=\frac{\sum_{i}^{n}(X_i - \bar{X})(X_i - \bar{X})}{n-1}=\frac{\sum_{i}^{n}(X_i - \bar{X})^2}{n-1}
$$

So, first,

$$\frac{\cov(X,Y)}{\var(X)}=\frac{\sum_{i}^{n}(X_i -
  \bar{X})(Y_i - \bar{Y})}{\sum_{i}^{n}(X_i - \bar{X})^2}$$

because
the (n-1) cancels out. (Thus, we had to divide $X^{T}X$ and $X^{T}y$
by n-1 in the sections above to get the analogous
covariances/variances). So, this is the bivariate case with
$y=\beta_0+\beta_1 x_1$. What about $y=\beta_0+\beta_1 x_1+ \beta_2 x_2$?

This becomes notationally messy fast.  Already,
however, you can get a sense for the idea of deviations from the mean
being a key ingredient in these calculations.

7.  Why might $\bX \bbeta$ be useful? Let's get back to the question of prediction. So far we have the a model that predicts future turnout with the following squared error. Please explain the following code.

```{r}
X<-as.matrix(cbind(constant=1,news.df[,c("medhhi1000","medage","blkpct","cands")]))
y<-news.df$rpre
bVector<-solve(t(X) %*% X) %*% t(X) %*% y
yhat<- X %*% bVector
errors<-news.df$r-yhat
summary(errors)
mseOLS<-mean(errors^2) 
```

Now, we suspect that we could do better than this from our reading in  @james2013introduction. Here is an example using the lasso. What do you think? Did we do a better job than OLS in this small dataset? What is going on in this code? Please explain the code.


```{r}
lassoCriterion<-function(lambda,b,y,X){
  ## Assumes that the first column of X is all 1s
  yhat<-X %*% b
  ehat<-y-yhat
  l1.penalty<-sum(abs(b[-1])) ## no penalty on intercept
  thessr<-sum(ehat^2)
  lassocrit<-thessr+lambda*l1.penalty
  return(lassocrit)
}
lassoCriterion(lambda=.5,b=rep(0,ncol(X)),y=y,X=X)
lassoSol<-optim(par=c(0,0,0,0,0),
             fn=lassoCriterion,
             method="BFGS",
             X=X,
             y=y,
	     lambda=.5,
             control=list(trace=1,REPORT=1))
yhatL1<-X %*% lassoSol$par
errorsL1<-news.df$r-yhatL1
mseL1<-mean(errorsL1^2)
## Now see if we can find the best value of lambda
nlambdas<-100
somelambdas<-seq(.001,100,length=nlambdas)
## A function to get lasso criterion coefs and MSE
getlassoMSE<-function(lambda,X,y){
  lassoSol<-optim(par=rep(0,ncol(X)),
		  fn=lassoCriterion,
		  method="BFGS",
		  X=X,
		  y=y,
		  lambda=lambda)
		  #,control=list(trace=1,REPORT=1))
  yhatL1<-X %*% lassoSol$par
  errorsL1<-news.df$r-yhatL1
  mseL1<-mean(errorsL1^2)
  return(c(par=lassoSol$par,mse=mseL1,lambda=lambda))
}
results<-sapply(somelambdas,function(l){ getlassoMSE(l,X=X,y=y) })
## apply(results,1,summary)
min(results["mse",])>mseOLS
results["lambda",results["mse",]==min(results["mse",])]
```

```{r, eval=FALSE}
## To do this even faster:
library(glmnet)
lassoFits<-glmnet(x=X[,-1],y=y,alpha=.5) ## using an elastic net fit rather than strict lasso
par(mar=c(6,2,3,1),mgp=c(1.5,.5,0))
plot(lassoFits,xvar="lambda",label=TRUE)
## This next line add the raw lambdas since the default plot shows only the log transformed lambdas
axis(1,line=2,at=log(lassoFits$lambda),labels=round(lassoFits$lambda,3))
abline(h=0,col="gray",lwd=.5)
```

```{r eval=FALSE}
getMSEs<-function(B,X,obsy){
  yhats <- apply(B,2,function(b){ X %*% b })
  ehats <- apply(yhats,2,function(y){ obsy - y })
  apply(ehats,2,function(e){ mean(e^2) })
}
getMSEs(B=coef(lassoFits),X=X,obsy=y)
```

Hmmm.... should the MSE always just go down? I wonder what @james2013introduction has to say about this.^[I suspect that @james2013introduction would recommend cross-validation --- but we only have 8 observations here, so that would be difficult.]

Now, more benefits of penalized models. Imagine this model:

```{r}
newmodel<-rpre ~ blkpct*medhhi1000*medage*cands
lm4<-lm(newmodel,news.df)
X<-model.matrix(newmodel,data=news.df)
try(b<-solve(t(X) %*% X) %*% X %*% y)
lsCriterion2<-function(b,y,X){
  yhat<-X %*% b
  ehat<-y-yhat
  thessr<-sum(ehat^2)
  return(thessr)
}
lsSolution1<-optim(fn=lsCriterion2,par=rep(0,ncol(X)),X=X,y=y,method="BFGS",control=list(trace=1,REPORT=1,maxit=5000))
lsSolution1
lsSolution2<-optim(fn=lsCriterion2,par=rep(100,ncol(X)),X=X,y=y,method="BFGS",control=list(trace=1,REPORT=1,maxit=5000))
lsSolution2
lsSolution3<-optim(fn=lsCriterion2,par=rep(-100,ncol(X)),X=X,y=y,method="BFGS",control=list(trace=1,REPORT=1,maxit=5000))
lsSolution3
cbind(lsSolution1$par,lsSolution2$par,lsSolution3$par)
## Notice that we are not standardizing the columns of X here. Should do that for real use.
ridgeCriterion<-function(lambda,b,y,X){
  ## Assumes that the first column of X is all 1s
  yhat<-X %*% b
  ehat<-y-yhat
  l2.penalty<-sum(b[-1]^2) ## no penalty on intercept
  thessr<-sum(ehat^2)
  lassocrit<-thessr+lambda*l2.penalty
  return(lassocrit)
}
ridgeSolution1<-optim(fn=ridgeCriterion,par=rep(0,ncol(X)),lambda=.5,X=X,y=y,method="BFGS",control=list(trace=1,REPORT=1,maxit=5000))
ridgeSolution1
ridgeSolution2<-optim(fn=ridgeCriterion,par=rep(100,ncol(X)),lambda=.5,X=X,y=y,method="BFGS",control=list(trace=1,REPORT=1,maxit=5000))
ridgeSolution2
ridgeSolution3<-optim(fn=ridgeCriterion,par=rep(-100,ncol(X)),lambda=.5,X=X,y=y,method="BFGS",control=list(trace=1,REPORT=1,maxit=5000))
ridgeSolution3
cbind(ridgeSolution1$par,ridgeSolution2$par,ridgeSolution3$par)
## A sketch of Cross-validation to choose lambda: here using 3-fold because of the small dataset
cvfn<-function(){
  testids<-sample(1:nrow(X),nrow(X)/3)
  trainingids <- (1:nrow(X))[-testids]
  ## Fit
  ## Predict yhat for testids
  ## MSE for y_test versus yhat_test
}
## Average of the MSE across folds is CV MSE
```
